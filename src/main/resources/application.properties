
spring.ai.ollama.base-url=http://localhost:11434


# Changing default LLM
embabel.models.default-llm=qwen3-coder:30b
#embabel.models.default-embedding-model=nomic-embed-text:latest

# Llm Roles: Create as many as you want and use with byRole("role-name")
embabel.models.llms.best=qwen3-coder:30b
embabel.models.llms.cheapest=gpt-oss:20b

#embabel.models.embedding-services.best=nomic-embed-text:latest
#embabel.models.embedding-services.cheapest=nomic-embed-text:latest

# Set this to use your preferred LLM for ranking, to avoid default
embabel.agent-platform.ranking.llm=gpt-oss:20b

# Coding assistant LLM defaults
assistant.llm.model=${embabel.models.default-llm:qwen3-coder:30b}
assistant.llm.fallback-model=${embabel.models.llms.cheapest:gpt-oss:20b}
assistant.llm.temperature.craft=0.7
assistant.llm.temperature.review=0.35
# Local-only mode also disables web search in SessionState, so no separate web-search flag is needed
assistant.local-only=true
